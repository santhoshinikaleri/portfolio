# robots.txt - Search Engine Crawler Instructions

User-agent: *
Allow: /
Disallow: 

# Crawl delay (seconds)
Crawl-delay: 1

# Sitemap location
Sitemap: https://yourusername.github.io/sitemap.xml

# Specific rules for search engines

# Google
User-agent: Googlebot
Allow: /

# Bing
User-agent: Bingbot
Allow: /

# Block unwanted bots
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /
